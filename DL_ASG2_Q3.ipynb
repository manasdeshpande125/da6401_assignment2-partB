{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpfkSZsCjSgAVfnaGsdEPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manasdeshpande125/da6401_assignment2-partB/blob/main/DL_ASG2_Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I have used ResNet18 pretrained model as it  has fewer layers so less complex training needed and less computation requirements**"
      ],
      "metadata": {
        "id": "GKQvp0dIgN2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Strategy1: Freezing All except Last**"
      ],
      "metadata": {
        "id": "Y2be_Zfwf7jO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mTdjKzYqJkG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# os.environ['WANDB_MODE'] = 'offline'\n",
        "# WandB logger setup\n",
        "wandb_logger = WandbLogger(project='DA6401-Assignment-2', name='fine_tune')\n",
        "\n",
        "# Define the model class\n",
        "class FineTuneResNet18(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=0.001, freeze_layers=True, num_classes=10):\n",
        "        super(FineTuneResNet18, self).__init__()\n",
        "        self.save_hyperparameters()  # Saves hyperparams to WandB\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "        if freeze_layers:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('train_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        val_loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        val_acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('val_loss', val_loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('val_acc', val_acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"val_loss\": val_loss, \"val_acc\": val_acc}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('test_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
        "\n",
        "\n",
        "# Load data\n",
        "def load_data(batch_size=32, data_aug='n'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomHorizontalFlip() if data_aug == 'y' else transforms.Lambda(lambda x: x),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(root='inaturalist_12K/train', transform=transform)\n",
        "    val_dataset = datasets.ImageFolder(root='inaturalist_12K/val', transform=transform)\n",
        "\n",
        "    val_size = int(0.2 * len(train_dataset))\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# Main training function\n",
        "def train_and_finetune(epochs=5, batch_size=32, data_aug='n', learning_rate=0.001):\n",
        "    train_loader, val_loader = load_data(batch_size, data_aug)\n",
        "\n",
        "    model = FineTuneResNet18(learning_rate=learning_rate, freeze_layers=True)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=\"checkpoints/\",\n",
        "        filename=\"finetune-{epoch:02d}-{val_loss:.2f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=1,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "\n",
        "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=epochs,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback],\n",
        "        logger=wandb_logger,\n",
        "        accelerator='auto',\n",
        "        devices=1 if torch.cuda.is_available() else None,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.test(model, val_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Kick off training\n",
        "finetuned_model = train_and_finetune(epochs=10, batch_size=32, data_aug='y', learning_rate=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Strategy2: UnFreezing Last K layers**"
      ],
      "metadata": {
        "id": "vhntkGbEhpQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['WANDB_MODE'] = 'offline'\n",
        "# WandB logger setup\n",
        "wandb_logger = WandbLogger(project='DA6401-Assignment-2', name='fine_tune1')\n",
        "\n",
        "class PartialFineTuneResNet18(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=0.001, unfreeze_from_layer=6, num_classes=10):\n",
        "        super(PartialFineTuneResNet18, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "        # Freeze all layers\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze layers from a certain layer onwards (e.g. layer4 and fc)\n",
        "        ct = 0\n",
        "        for child in self.model.children():\n",
        "            if ct >= unfreeze_from_layer:  # Unfreeze from this layer onwards\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "            ct += 1\n",
        "\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()),\n",
        "                         lr=self.learning_rate, momentum=0.9)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('train_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        val_loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        val_acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('val_loss', val_loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('val_acc', val_acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"val_loss\": val_loss, \"val_acc\": val_acc}\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('test_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
        "# Kick off training\n",
        "# Load data\n",
        "def load_data(batch_size=32, data_aug='n'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomHorizontalFlip() if data_aug == 'y' else transforms.Lambda(lambda x: x),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(root='inaturalist_12K/train', transform=transform)\n",
        "    val_dataset = datasets.ImageFolder(root='inaturalist_12K/val', transform=transform)\n",
        "\n",
        "    val_size = int(0.2 * len(train_dataset))\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Main training function\n",
        "def train_and_finetune(epochs=5, batch_size=32, data_aug='n', learning_rate=0.001):\n",
        "    train_loader, val_loader = load_data(batch_size, data_aug)\n",
        "\n",
        "    model = PartialFineTuneResNet18(learning_rate=learning_rate)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=\"checkpoints/\",\n",
        "        filename=\"finetune-{epoch:02d}-{val_loss:.2f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=1,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "\n",
        "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=epochs,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback],\n",
        "        logger=wandb_logger,\n",
        "        accelerator='auto',\n",
        "        devices=1 if torch.cuda.is_available() else None,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.test(model, val_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "finetuned_model = train_and_finetune(epochs=10, batch_size=32, data_aug='y', learning_rate=0.001)"
      ],
      "metadata": {
        "id": "Mo4D-zWeqaPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Strategy3: Gradual Unfreezing acorss epochs**"
      ],
      "metadata": {
        "id": "vozve8H_hveT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ['WANDB_MODE'] = 'offline'\n",
        "# WandB logger setup\n",
        "wandb_logger = WandbLogger(project='DA6401-Assignment-2', name='fine_tune2')\n",
        "\n",
        "class GradualUnfreezeResNet18(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=1e-3, num_classes=10):\n",
        "        super(GradualUnfreezeResNet18, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "        # Freeze all layers initially\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze only the FC layer\n",
        "        for param in self.model.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "        # Track which layer to unfreeze next\n",
        "        self.unfreeze_schedule = [\n",
        "            self.model.layer4,  # unfreeze in epoch 2\n",
        "            self.model.layer3,  # unfreeze in epoch 3\n",
        "            self.model.layer2,  # unfreeze in epoch 4\n",
        "        ]\n",
        "        self.unfreeze_index = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Different learning rates for different parts\n",
        "        param_groups = [\n",
        "            {\"params\": self.model.fc.parameters(), \"lr\": self.learning_rate * 1.0},\n",
        "            {\"params\": self.model.layer4.parameters(), \"lr\": self.learning_rate * 0.5},\n",
        "            {\"params\": self.model.layer3.parameters(), \"lr\": self.learning_rate * 0.1},\n",
        "            {\"params\": self.model.layer2.parameters(), \"lr\": self.learning_rate * 0.05},\n",
        "        ]\n",
        "        return optim.SGD(param_groups, momentum=0.9)\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        # Gradually unfreeze layers every epoch\n",
        "        if self.unfreeze_index < len(self.unfreeze_schedule):\n",
        "            for param in self.unfreeze_schedule[self.unfreeze_index].parameters():\n",
        "                param.requires_grad = True\n",
        "            self.unfreeze_index += 1\n",
        "            print(f\"Unfroze layer: {self.unfreeze_index}\")\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('train_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        val_loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        val_acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('val_loss', val_loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('val_acc', val_acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"val_loss\": val_loss, \"val_acc\": val_acc}\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('test_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
        "\n",
        "# Load data\n",
        "def load_data(batch_size=32, data_aug='n'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomHorizontalFlip() if data_aug == 'y' else transforms.Lambda(lambda x: x),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(root='inaturalist_12K/train', transform=transform)\n",
        "    val_dataset = datasets.ImageFolder(root='inaturalist_12K/val', transform=transform)\n",
        "\n",
        "    val_size = int(0.2 * len(train_dataset))\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_and_finetune(epochs=5, batch_size=32, data_aug='n', learning_rate=0.001):\n",
        "    train_loader, val_loader = load_data(batch_size, data_aug)\n",
        "\n",
        "    model = GradualUnfreezeResNet18(learning_rate=learning_rate)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=\"checkpoints/\",\n",
        "        filename=\"finetune-{epoch:02d}-{val_loss:.2f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=1,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "\n",
        "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=epochs,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback],\n",
        "        logger=wandb_logger,\n",
        "        accelerator='auto',\n",
        "        devices=1 if torch.cuda.is_available() else None,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.test(model, val_loader)\n",
        "\n",
        "    return model\n",
        "# Kick off training\n",
        "finetuned_model = train_and_finetune(epochs=10, batch_size=32, data_aug='y', learning_rate=0.001)\n"
      ],
      "metadata": {
        "id": "BD7QlEfJqccE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Strategy4: Train from Scratch**"
      ],
      "metadata": {
        "id": "G3fDu4gZh4I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['WANDB_MODE'] = 'offline'\n",
        "# WandB logger setup\n",
        "wandb_logger = WandbLogger(project='DA6401-Assignment-2', name='fine_tune3')\n",
        "\n",
        "class TrainResNet18FromScratch(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=0.001, num_classes=10):\n",
        "        super(TrainResNet18FromScratch, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize model WITHOUT pre-trained weights\n",
        "        self.model = torchvision.models.resnet18(pretrained=False)\n",
        "\n",
        "        # Replace final fully connected layer\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('train_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        val_loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        val_acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('val_loss', val_loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('val_acc', val_acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"val_loss\": val_loss, \"val_acc\": val_acc}\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('test_acc', acc, prog_bar=True, on_epoch=True)\n",
        "        return {\"test_loss\": loss, \"test_acc\": acc}\n",
        "\n",
        "\n",
        "def train_and_finetune(epochs=5, batch_size=32, data_aug='n', learning_rate=0.001):\n",
        "    train_loader, val_loader = load_data(batch_size, data_aug)\n",
        "\n",
        "    model = TrainResNet18FromScratch(learning_rate=learning_rate)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=\"checkpoints/\",\n",
        "        filename=\"finetune-{epoch:02d}-{val_loss:.2f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        mode=\"min\",\n",
        "        save_top_k=1,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "\n",
        "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=epochs,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback],\n",
        "        logger=wandb_logger,\n",
        "        accelerator='auto',\n",
        "        devices=1 if torch.cuda.is_available() else None,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    trainer.test(model, val_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "finetuned_model = train_and_finetune(epochs=10, batch_size=32, data_aug='y', learning_rate=0.001)\n"
      ],
      "metadata": {
        "id": "nUlUUSECqevS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}